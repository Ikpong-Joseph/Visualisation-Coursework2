{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729e1362-dd09-4b5d-8a1e-8f66dde745f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"UK_Accident.csv\")      # full dataset from https://www.kaggle.com/datasets/devansodariya/road-accident-united-kingdom-uk-dataset\n",
    "df_first20k = df.iloc[:100_000]                # slice out the first 100 000 rows as original \"UK_Accident.csv\" has > 1M entries\n",
    "# or equivalently:\n",
    "# df_first20k = df.head(100_000)\n",
    "\n",
    "df_first20k.to_csv(\"first_100k_rows_UK_Accident.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb8fbf43-451c-41c9-851a-bbbe597d7274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 1504150 rows, 33 columns\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1504150 entries, 0 to 1504149\n",
      "Data columns (total 33 columns):\n",
      " #   Column                                       Non-Null Count    Dtype  \n",
      "---  ------                                       --------------    -----  \n",
      " 0   Unnamed: 0                                   1504150 non-null  int64  \n",
      " 1   Accident_Index                               1504150 non-null  object \n",
      " 2   Location_Easting_OSGR                        1504049 non-null  float64\n",
      " 3   Location_Northing_OSGR                       1504150 non-null  float64\n",
      " 4   Longitude                                    1504049 non-null  float64\n",
      " 5   Latitude                                     1504150 non-null  float64\n",
      " 6   Police_Force                                 1504150 non-null  int64  \n",
      " 7   Accident_Severity                            1504150 non-null  int64  \n",
      " 8   Number_of_Vehicles                           1504150 non-null  int64  \n",
      " 9   Number_of_Casualties                         1504150 non-null  int64  \n",
      " 10  Date                                         1504150 non-null  object \n",
      " 11  Day_of_Week                                  1504150 non-null  int64  \n",
      " 12  Time                                         1504033 non-null  object \n",
      " 13  Local_Authority_(District)                   1504150 non-null  int64  \n",
      " 14  Local_Authority_(Highway)                    1504150 non-null  object \n",
      " 15  1st_Road_Class                               1504150 non-null  int64  \n",
      " 16  1st_Road_Number                              1504150 non-null  int64  \n",
      " 17  Road_Type                                    1504150 non-null  object \n",
      " 18  Speed_limit                                  1504150 non-null  int64  \n",
      " 19  Junction_Control                             901315 non-null   object \n",
      " 20  2nd_Road_Class                               1504150 non-null  int64  \n",
      " 21  2nd_Road_Number                              1504150 non-null  int64  \n",
      " 22  Pedestrian_Crossing-Human_Control            1504133 non-null  object \n",
      " 23  Pedestrian_Crossing-Physical_Facilities      1504116 non-null  object \n",
      " 24  Light_Conditions                             1504150 non-null  object \n",
      " 25  Weather_Conditions                           1504150 non-null  object \n",
      " 26  Road_Surface_Conditions                      1504150 non-null  object \n",
      " 27  Special_Conditions_at_Site                   36582 non-null    object \n",
      " 28  Carriageway_Hazards                          27250 non-null    object \n",
      " 29  Urban_or_Rural_Area                          1504150 non-null  int64  \n",
      " 30  Did_Police_Officer_Attend_Scene_of_Accident  1504150 non-null  object \n",
      " 31  LSOA_of_Accident_Location                    1395912 non-null  object \n",
      " 32  Year                                         1504150 non-null  int64  \n",
      "dtypes: float64(4), int64(14), object(15)\n",
      "memory usage: 378.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "# Provide details of your dataset. For example, the number of rows,\n",
    "# number of columns, name, and type of each attribute.\n",
    "\n",
    "# Load the all rows of the dataset\n",
    "df = pd.read_csv(\"UK_Accident.csv\")\n",
    "\n",
    "# Prepare summary of columns and data types\n",
    "schema_df = pd.DataFrame({\n",
    "    'Attribute Name': df.columns,\n",
    "    'Data Type': df.dtypes.astype(str)\n",
    "})\n",
    "# display(schema_df)\n",
    "\n",
    "# Display the schema to the user\n",
    "#!pip install pandas ace_tools\n",
    "# import ace_tools as tools\n",
    "from IPython.display import display\n",
    "\n",
    "# tools.display_dataframe_to_user(name=\"Dataset Schema and Types\", dataframe=schema_df)\n",
    "\n",
    "# Also, print the overall shape\n",
    "print(f\"Dataset shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "df.info()  # prints names, non-null counts, and dtypes summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed663506-a44f-4076-baa5-f725eda3f2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 100000 rows, 33 columns\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 33 columns):\n",
      " #   Column                                       Non-Null Count   Dtype  \n",
      "---  ------                                       --------------   -----  \n",
      " 0   Unnamed: 0                                   100000 non-null  int64  \n",
      " 1   Accident_Index                               100000 non-null  object \n",
      " 2   Location_Easting_OSGR                        99969 non-null   float64\n",
      " 3   Location_Northing_OSGR                       100000 non-null  float64\n",
      " 4   Longitude                                    99969 non-null   float64\n",
      " 5   Latitude                                     100000 non-null  float64\n",
      " 6   Police_Force                                 100000 non-null  int64  \n",
      " 7   Accident_Severity                            100000 non-null  int64  \n",
      " 8   Number_of_Vehicles                           100000 non-null  int64  \n",
      " 9   Number_of_Casualties                         100000 non-null  int64  \n",
      " 10  Date                                         100000 non-null  object \n",
      " 11  Day_of_Week                                  100000 non-null  int64  \n",
      " 12  Time                                         99999 non-null   object \n",
      " 13  Local_Authority_(District)                   100000 non-null  int64  \n",
      " 14  Local_Authority_(Highway)                    100000 non-null  object \n",
      " 15  1st_Road_Class                               100000 non-null  int64  \n",
      " 16  1st_Road_Number                              100000 non-null  int64  \n",
      " 17  Road_Type                                    100000 non-null  object \n",
      " 18  Speed_limit                                  100000 non-null  int64  \n",
      " 19  Junction_Control                             64378 non-null   object \n",
      " 20  2nd_Road_Class                               100000 non-null  int64  \n",
      " 21  2nd_Road_Number                              100000 non-null  int64  \n",
      " 22  Pedestrian_Crossing-Human_Control            99987 non-null   object \n",
      " 23  Pedestrian_Crossing-Physical_Facilities      99983 non-null   object \n",
      " 24  Light_Conditions                             100000 non-null  object \n",
      " 25  Weather_Conditions                           100000 non-null  object \n",
      " 26  Road_Surface_Conditions                      100000 non-null  object \n",
      " 27  Special_Conditions_at_Site                   2271 non-null    object \n",
      " 28  Carriageway_Hazards                          1569 non-null    object \n",
      " 29  Urban_or_Rural_Area                          100000 non-null  int64  \n",
      " 30  Did_Police_Officer_Attend_Scene_of_Accident  100000 non-null  object \n",
      " 31  LSOA_of_Accident_Location                    99889 non-null   object \n",
      " 32  Year                                         100000 non-null  int64  \n",
      "dtypes: float64(4), int64(14), object(15)\n",
      "memory usage: 25.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "# Provide details of your dataset. For example, the number of rows,\n",
    "# number of columns, name, and type of each attribute.\n",
    "\n",
    "# Load the first 100k rows of the dataset\n",
    "df = pd.read_csv('first_100k_rows_UK_Accident.csv')\n",
    "\n",
    "# Prepare summary of columns and data types\n",
    "schema_df = pd.DataFrame({\n",
    "    'Attribute Name': df.columns,\n",
    "    'Data Type': df.dtypes.astype(str)\n",
    "})\n",
    "# display(schema_df)\n",
    "\n",
    "# Display the schema to the user\n",
    "#!pip install pandas ace_tools\n",
    "# import ace_tools as tools\n",
    "from IPython.display import display\n",
    "\n",
    "# tools.display_dataframe_to_user(name=\"Dataset Schema and Types\", dataframe=schema_df)\n",
    "\n",
    "# Also, print the overall shape\n",
    "print(f\"Dataset shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "df.info()  # prints names, non-null counts, and dtypes summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5206de-2b28-4257-a321-ee762e5a8790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab5d7b7-d0be-4fcf-9cd1-d56668258313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carriageway_Hazards                        0.98431\n",
      "Special_Conditions_at_Site                 0.97729\n",
      "Junction_Control                           0.35622\n",
      "LSOA_of_Accident_Location                  0.00111\n",
      "Location_Easting_OSGR                      0.00031\n",
      "Longitude                                  0.00031\n",
      "Pedestrian_Crossing-Physical_Facilities    0.00017\n",
      "Pedestrian_Crossing-Human_Control          0.00013\n",
      "Time                                       0.00001\n",
      "Unnamed: 0                                 0.00000\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna(df[c].median(), inplace=True)\n",
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Temp\\ipykernel_23788\\3839849664.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].fillna('Unknown', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✔ Saved: first_100k_rows_UK_Accident_missing_values_handling.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Missing-Value Handling\n",
    "\n",
    "# Load the first 100k rows of the dataset\n",
    "df = pd.read_csv('first_100k_rows_UK_Accident.csv')\n",
    "\n",
    "# 1. Calculate the fraction of missing values per column\n",
    "missing_pct = df.isna().mean().sort_values(ascending=False)\n",
    "#   - df.isna() creates a boolean DataFrame (True where value is NaN)\n",
    "#   - .mean() computes the column-wise mean of those booleans (i.e. % missing)\n",
    "#   - .sort_values(ascending=False) orders columns from most to least missing\n",
    "\n",
    "# 2. Display the top 10 columns by missingness\n",
    "print(missing_pct.head(10))  # shows names and % missing for the 10 worst offenders\n",
    "\n",
    "# 3. Identify columns to drop: those with >80% missing\n",
    "to_drop = missing_pct[missing_pct > 0.80].index.tolist()\n",
    "#   - missing_pct > 0.80 gives a boolean Series where True means >80% missing\n",
    "#   - .index extracts the column names, .tolist() makes a Python list\n",
    "\n",
    "# 4. Drop those nearly-empty columns in-place\n",
    "df.drop(columns=to_drop, inplace=True)\n",
    "#   - columns=to_drop tells pandas which to remove\n",
    "#   - inplace=True updates df directly (no new copy)\n",
    "\n",
    "# 5. Define critical columns where missing values are unacceptable\n",
    "critical = ['Date','Time','Latitude','Longitude','Accident_Severity']\n",
    "#   - If these go missing, the row loses essential context\n",
    "\n",
    "# 6. Drop any row missing in one of those critical fields\n",
    "df.dropna(subset=critical, inplace=True)\n",
    "#   - subset=critical ensures only rows lacking at least one of these are removed\n",
    "\n",
    "# 7. Find all remaining numeric columns for median imputation\n",
    "num_cols = df.select_dtypes(include=['int64','float64']).columns\n",
    "#   - select_dtypes picks columns by dtype, .columns returns their names\n",
    "\n",
    "# 8. Loop through numeric columns, filling NaNs with the column’s median\n",
    "for c in num_cols:\n",
    "    df[c].fillna(df[c].median(), inplace=True)\n",
    "    # - df[c].median() calculates the middle value\n",
    "    # - fillna(..., inplace=True) replaces NaNs with that median\n",
    "\n",
    "# 9. Find all remaining categorical columns for mode/“Unknown” fill\n",
    "cat_cols = df.select_dtypes(include=['object','category']).columns\n",
    "#   - object/category dtypes typically represent strings or categorical codes\n",
    "\n",
    "# 10. Loop through categorical columns, replacing NaNs with 'Unknown'\n",
    "for c in cat_cols:\n",
    "    df[c].fillna('Unknown', inplace=True)\n",
    "    # - 'Unknown' acts as a catch-all category for missing labels\n",
    "\n",
    "\n",
    "df.to_csv('first_100k_rows_UK_Accident_missing_values_handling.csv', index=False)\n",
    "print((\"  ✔ Saved: first_100k_rows_UK_Accident_missing_values_handling.csv\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c789eb6e-5400-44ed-9029-ffa4738e2e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✔ Saved: first_100k_rows_UK_Accident_date_parsing.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2: Datetime Parsing & Temporal Features\n",
    "\n",
    "# import pandas as pd\n",
    "# Date is in dd/mm/yyyy format.\n",
    "\n",
    "# Day_of_Week runs from 1–7 (no zeroes). In the UK DfT schema, 1 = Sunday, 2 = Monday, …, 7 = Saturday.\n",
    "\n",
    "df = pd.read_csv('first_100k_rows_UK_Accident_missing_values_handling.csv')\n",
    "# 1. Parse Date and Time into a single Timestamp\n",
    "df['Datetime'] = pd.to_datetime(\n",
    "    df['Date'] + ' ' + df['Time'],\n",
    "    dayfirst=True,                  # interprets dd/mm/yyyy\n",
    "    format='%d/%m/%Y %H:%M'         # speeds up parsing\n",
    ")\n",
    "\n",
    "# 2. Map Day_of_Week → names\n",
    "dow_map = {\n",
    "    1: 'Sunday', 2: 'Monday', 3: 'Tuesday', 4: 'Wednesday',\n",
    "    5: 'Thursday', 6: 'Friday', 7: 'Saturday'\n",
    "}\n",
    "df['Day_Name'] = df['Day_of_Week'].map(dow_map)\n",
    "\n",
    "# 3. Extract numeric temporal features\n",
    "df['Hour']         = df['Datetime'].dt.hour         # 0–23\n",
    "df['Month']        = df['Datetime'].dt.month        # 1–12\n",
    "df['DayOfMonth']   = df['Datetime'].dt.day          # 1–31\n",
    "df['Is_Weekend']   = df['Day_Name'].isin(['Saturday','Sunday']).astype(int)\n",
    "\n",
    "# Seasonal bucket\n",
    "season_map = {\n",
    "    12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "     3: 'Spring',4: 'Spring',5: 'Spring',\n",
    "     6: 'Summer',7: 'Summer',8: 'Summer',\n",
    "     9: 'Autumn',10: 'Autumn',11: 'Autumn'\n",
    "}\n",
    "df['Season'] = df['Month'].map(season_map)\n",
    "\n",
    "df.to_csv('first_100k_rows_UK_Accident_date_parsing.csv', index=False)\n",
    "print((\"  ✔ Saved: first_100k_rows_UK_Accident_date_parsing.csv\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aac22c9-9d57-4f35-b872-8de718d2f4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99968 entries, 0 to 99967\n",
      "Data columns (total 38 columns):\n",
      " #   Column                                       Non-Null Count  Dtype  \n",
      "---  ------                                       --------------  -----  \n",
      " 0   Unnamed: 0                                   99968 non-null  int64  \n",
      " 1   Accident_Index                               99968 non-null  object \n",
      " 2   Location_Easting_OSGR                        99968 non-null  float64\n",
      " 3   Location_Northing_OSGR                       99968 non-null  float64\n",
      " 4   Longitude                                    99968 non-null  float64\n",
      " 5   Latitude                                     99968 non-null  float64\n",
      " 6   Police_Force                                 99968 non-null  int64  \n",
      " 7   Accident_Severity                            99968 non-null  int64  \n",
      " 8   Number_of_Vehicles                           99968 non-null  int64  \n",
      " 9   Number_of_Casualties                         99968 non-null  int64  \n",
      " 10  Date                                         99968 non-null  object \n",
      " 11  Day_of_Week                                  99968 non-null  int64  \n",
      " 12  Time                                         99968 non-null  object \n",
      " 13  Local_Authority_(District)                   99968 non-null  int64  \n",
      " 14  Local_Authority_(Highway)                    99968 non-null  object \n",
      " 15  1st_Road_Class                               99968 non-null  int64  \n",
      " 16  1st_Road_Number                              99968 non-null  int64  \n",
      " 17  Road_Type                                    99968 non-null  object \n",
      " 18  Speed_limit                                  99968 non-null  int64  \n",
      " 19  Junction_Control                             99968 non-null  object \n",
      " 20  2nd_Road_Class                               99968 non-null  int64  \n",
      " 21  2nd_Road_Number                              99968 non-null  int64  \n",
      " 22  Pedestrian_Crossing-Human_Control            99968 non-null  object \n",
      " 23  Pedestrian_Crossing-Physical_Facilities      99968 non-null  object \n",
      " 24  Light_Conditions                             99968 non-null  object \n",
      " 25  Weather_Conditions                           99968 non-null  object \n",
      " 26  Road_Surface_Conditions                      99968 non-null  object \n",
      " 27  Urban_or_Rural_Area                          99968 non-null  int64  \n",
      " 28  Did_Police_Officer_Attend_Scene_of_Accident  99968 non-null  object \n",
      " 29  LSOA_of_Accident_Location                    99968 non-null  object \n",
      " 30  Year                                         99968 non-null  int64  \n",
      " 31  Datetime                                     99968 non-null  object \n",
      " 32  Day_Name                                     99968 non-null  object \n",
      " 33  Hour                                         99968 non-null  int64  \n",
      " 34  Month                                        99968 non-null  int64  \n",
      " 35  DayOfMonth                                   99968 non-null  int64  \n",
      " 36  Is_Weekend                                   99968 non-null  int64  \n",
      " 37  Season                                       99968 non-null  object \n",
      "dtypes: float64(4), int64(18), object(16)\n",
      "memory usage: 29.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# 3. Categorical Encoding\n",
    "df = pd.read_csv('first_100k_rows_UK_Accident_date_parsing.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d7a1d5b-e822-4fd2-a3da-f4e4660575b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✔ Saved: first_100k_rows_UK_Accident_encoded.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Read in the interim CSV that already has dates parsed\n",
    "df = pd.read_csv('first_100k_rows_UK_Accident_date_parsing.csv')\n",
    "\n",
    "# 2. Map the ordinal target ‘Accident_Severity’ into a new ordered numeric\n",
    "#    Here we assume 1=Fatal (highest), 2=Serious, 3=Minor (lowest risk)\n",
    "severity_map = {1: 3, 2: 2, 3: 1}\n",
    "df['Severity_Ord'] = df['Accident_Severity'].map(severity_map)\n",
    "#   - .map() replaces each original code with its mapped value\n",
    "#   - Creates ‘Severity_Ord’ so models can interpret severity as an ordinal feature\n",
    "\n",
    "text_map = {\n",
    "    'Fine without high winds':      'Clear',\n",
    "    'Raining without high winds':   'Rain',\n",
    "    'Raining with high winds':      'Rain',\n",
    "    'Snowing without high winds':   'Snow/Ice',\n",
    "    'Snowing with high winds':   'Snow/Ice',\n",
    "    'Fog or mist':                  'Fog',\n",
    "}\n",
    "df['Weather_Simple'] = (\n",
    "    df['Weather_Conditions']  # original text column\n",
    "      .map(text_map)          # map to our 6 buckets\n",
    "      .fillna('Other')        # catch-all for anything else/missing\n",
    ")\n",
    "\n",
    "# 3. One-hot encode purely nominal categorical variables\n",
    "nominal = ['Light_Conditions', 'Weather_Simple', 'Road_Surface_Conditions', 'Urban_or_Rural_Area']\n",
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=nominal,   # Which columns to expand into dummy variables\n",
    "    drop_first=True    # Drop the first category per variable to avoid multicollinearity\n",
    ")\n",
    "#   - Produces new binary columns like Light_Conditions_Darkness, etc.\n",
    "#   - Ensures no artificial ordering is imposed on these categories\n",
    "\n",
    "# 4. Import the scaler for numeric standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 5. Specify which continuous features need scaling\n",
    "scale_cols = ['Speed_limit', 'Number_of_Vehicles', 'Number_of_Casualties']\n",
    "scaler = StandardScaler() #ZScore\n",
    "#   - StandardScaler subtracts the mean and divides by the standard deviation\n",
    "#   - for the majority of classical ML algorithms, StandardScaler is a solid default.\n",
    "\n",
    "# 6. Fit to data and transform in one step, replacing raw values with z-scores\n",
    "df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
    "#   - fit_transform() learns the mean/std on these columns and applies the transformation\n",
    "#   - Ensures these features are on comparable scales (mean = 0, std = 1)\n",
    "\n",
    "# 7. Save the fully encoded & scaled DataFrame for downstream modeling\n",
    "df.to_csv('first_100k_rows_UK_Accident_encoded.csv', index=False)\n",
    "print(\"  ✔ Saved: first_100k_rows_UK_Accident_encoded.csv\")\n",
    "#   - index=False omits the DataFrame’s row index from the output file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1734a31-1de5-43a7-a709-9c8009ae0f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ikpong Joseph\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\Ikpong Joseph\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved: first_100k_rows_UK_Accident_feature_engineered.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = pd.read_csv('first_100k_rows_UK_Accident_encoded.csv')\n",
    "\n",
    "# 5a. Time buckets: cut the hour-of-day into meaningful traffic periods\n",
    "#   → creates a new categorical column ‘Time_Bucket’\n",
    "\n",
    "bins   = [0, 6, 9, 16, 19, 24]                          # define the edges of our buckets\n",
    "labels = ['Night','AM_Rush','Daytime','PM_Rush','Evening']  # labels for each bucket\n",
    "df['Time_Bucket'] = pd.cut(\n",
    "    df['Hour'],                # the source column\n",
    "    bins=bins,                 # bin edges\n",
    "    labels=labels,             # names for each bin\n",
    "    right=False                # include left edge, exclude right\n",
    ")\n",
    "# When you pass a list of edges into pd.cut, you’re defining interval boundaries, not one boundary per category.\n",
    "# If you give n edges, you automatically get n – 1 intervals between them, \n",
    "# which is exactly the number of labels you need.\n",
    "'''\n",
    "| Interval | Label    |   Interval --> bins\n",
    "| -------- | -------- |   Label --> labels\n",
    "| [0, 6)   | Night    |\n",
    "| [6, 9)   | AM_Rush  |\n",
    "| [9, 16)  | Daytime  |\n",
    "| [16, 19) | PM_Rush  |\n",
    "| [19, 24) | Evening  |\n",
    "\n",
    "'''\n",
    "# 5c. Geospatial clustering: find accident “hotspots”\n",
    "coords = df[['Latitude','Longitude']].dropna()  # grab only rows with both coords\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters, fixed seed\n",
    "labels = kmeans.fit_predict(coords)              # assign each point to a cluster\n",
    "df.loc[coords.index, 'Location_Cluster'] = labels  \n",
    "#   → new int column 0–9 indicating cluster membership\n",
    "\n",
    "# 5d. Interaction term: high speed at night might be especially risky\n",
    "df['Speed_x_Night'] = (\n",
    "    df['Speed_limit']                          # standardized speed, zscore of the original raw Speed_limit\n",
    "    * (df['Time_Bucket'] == 'Night').astype(int)  # 1 if Night, else 0\n",
    ")\n",
    "'''\n",
    "Contents in row Speed_x_Night\n",
    "\tSpeed_x_Night\n",
    "1\t-0.0\n",
    "2\t-0.0\n",
    "3\t-0.515787010885552\n",
    "4\t-0.0\n",
    "5\t-0.0\n",
    "6\t-0.0\n",
    "7\t-0.0\n",
    "8\t-0.0\n",
    "9\t-0.0\n",
    "10\t-0.0\n",
    "11\t-0.515787010885552\n",
    "12\t-0.0\n",
    "13\t-0.0\n",
    "\n",
    "This is because Speed_limit is the scaled value (Z-Score) of the original Speed_limit\n",
    "This also means that If a given night-time accident had a speed_limit below the dataset’s mean, \n",
    "its z-score is negative (e.g. −0.5158). \n",
    "Multiplying that by the Night indicator (1) leaves you with −0.5158.\n",
    "The “night indicator” isn’t an original column in the CSV\n",
    "it’s created on the fly from the Time_Bucket feature engineered in step 5a above\n",
    "The line ```df['Time_Bucket'] == 'Night').astype(int)``` \n",
    "produces a Boolean Series that’s True when the ['Time_Bucket'] == 'Night',\n",
    "False otherwise, and the .astype(int) converts it to 1 and 0.\n",
    "Thus the 0 and 0.nnn...\n",
    "'''\n",
    "#   → numeric feature that’s speed when night, else 0\n",
    "\n",
    "#  → captures broad seasonal variation (e.g. icy roads in Winter)\n",
    "\n",
    "# Season as a one-hot variable too:\n",
    "# df = pd.get_dummies(df, columns=['Season'], drop_first=True)\n",
    "\n",
    "# 5g. Save the feature-engineered file for downstream tasks\n",
    "df.to_csv('first_100k_rows_UK_Accident_feature_engineered.csv', index=False)\n",
    "print(\"✔ Saved: first_100k_rows_UK_Accident_feature_engineered.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbda47fb-b54a-4a34-873e-967ec77ed461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>Location_Easting_OSGR</th>\n",
       "      <th>Location_Northing_OSGR</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Police_Force</th>\n",
       "      <th>Accident_Severity</th>\n",
       "      <th>Number_of_Vehicles</th>\n",
       "      <th>Number_of_Casualties</th>\n",
       "      <th>...</th>\n",
       "      <th>Road_Surface_Conditions_Flood (Over 3cm of water)</th>\n",
       "      <th>Road_Surface_Conditions_Frost/Ice</th>\n",
       "      <th>Road_Surface_Conditions_Normal</th>\n",
       "      <th>Road_Surface_Conditions_Snow</th>\n",
       "      <th>Road_Surface_Conditions_Wet/Damp</th>\n",
       "      <th>Urban_or_Rural_Area_2</th>\n",
       "      <th>Urban_or_Rural_Area_3</th>\n",
       "      <th>Time_Bucket</th>\n",
       "      <th>Location_Cluster</th>\n",
       "      <th>Speed_x_Night</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>200501BS00001</td>\n",
       "      <td>525680.0</td>\n",
       "      <td>178240.0</td>\n",
       "      <td>-0.191170</td>\n",
       "      <td>51.489096</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.201681</td>\n",
       "      <td>-0.444252</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>PM_Rush</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>200501BS00002</td>\n",
       "      <td>524170.0</td>\n",
       "      <td>181650.0</td>\n",
       "      <td>-0.211708</td>\n",
       "      <td>51.520075</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.201681</td>\n",
       "      <td>-0.444252</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>PM_Rush</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>200501BS00003</td>\n",
       "      <td>524520.0</td>\n",
       "      <td>182240.0</td>\n",
       "      <td>-0.206458</td>\n",
       "      <td>51.525301</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.226971</td>\n",
       "      <td>-0.444252</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.515787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>200501BS00004</td>\n",
       "      <td>526900.0</td>\n",
       "      <td>177530.0</td>\n",
       "      <td>-0.173862</td>\n",
       "      <td>51.482442</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.201681</td>\n",
       "      <td>-0.444252</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Daytime</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Accident_Index  Location_Easting_OSGR  Location_Northing_OSGR  \\\n",
       "0           0  200501BS00001               525680.0                178240.0   \n",
       "1           1  200501BS00002               524170.0                181650.0   \n",
       "2           2  200501BS00003               524520.0                182240.0   \n",
       "3           3  200501BS00004               526900.0                177530.0   \n",
       "\n",
       "   Longitude   Latitude  Police_Force  Accident_Severity  Number_of_Vehicles  \\\n",
       "0  -0.191170  51.489096             1                  2           -1.201681   \n",
       "1  -0.211708  51.520075             1                  3           -1.201681   \n",
       "2  -0.206458  51.525301             1                  3            0.226971   \n",
       "3  -0.173862  51.482442             1                  3           -1.201681   \n",
       "\n",
       "   Number_of_Casualties  ...  \\\n",
       "0             -0.444252  ...   \n",
       "1             -0.444252  ...   \n",
       "2             -0.444252  ...   \n",
       "3             -0.444252  ...   \n",
       "\n",
       "  Road_Surface_Conditions_Flood (Over 3cm of water)  \\\n",
       "0                                             False   \n",
       "1                                             False   \n",
       "2                                             False   \n",
       "3                                             False   \n",
       "\n",
       "   Road_Surface_Conditions_Frost/Ice Road_Surface_Conditions_Normal  \\\n",
       "0                              False                          False   \n",
       "1                              False                          False   \n",
       "2                              False                          False   \n",
       "3                              False                          False   \n",
       "\n",
       "   Road_Surface_Conditions_Snow Road_Surface_Conditions_Wet/Damp  \\\n",
       "0                         False                             True   \n",
       "1                         False                            False   \n",
       "2                         False                            False   \n",
       "3                         False                            False   \n",
       "\n",
       "   Urban_or_Rural_Area_2  Urban_or_Rural_Area_3 Time_Bucket  Location_Cluster  \\\n",
       "0                  False                  False     PM_Rush               1.0   \n",
       "1                  False                  False     PM_Rush               1.0   \n",
       "2                  False                  False       Night               1.0   \n",
       "3                  False                  False     Daytime               1.0   \n",
       "\n",
       "  Speed_x_Night  \n",
       "0     -0.000000  \n",
       "1     -0.000000  \n",
       "2     -0.515787  \n",
       "3     -0.000000  \n",
       "\n",
       "[4 rows x 54 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('first_100k_rows_UK_Accident_feature_engineered.csv')\n",
    "df.head(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67c9e6-8973-4355-9d00-65d1e1cf38cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
